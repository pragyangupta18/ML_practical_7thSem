{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# 1. Set up the environment\n",
        "def display_board(board):\n",
        "    for row in board:\n",
        "        print(\"-\" * 9)\n",
        "        print(\" | \".join([\"X\" if cell == 1 else \"O\" if cell == -1 else \" \" for cell in row]))\n",
        "    print(\"-\" * 9)\n",
        "\n",
        "def is_valid_move(board, row, col):\n",
        "    return board[row][col] == 0\n",
        "\n",
        "def check_win(board, player):\n",
        "    # Check rows, columns, and diagonals for a win\n",
        "    for row in board:\n",
        "        if all(cell == player for cell in row):\n",
        "            return True\n",
        "    for col in board.T:\n",
        "        if all(cell == player for cell in col):\n",
        "            return True\n",
        "    if all(board[i][i] == player for i in range(3)) or all(board[i][2 - i] == player for i in range(3)):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def check_draw(board):\n",
        "    return np.all(board != 0)\n",
        "\n",
        "# 2. Define the Tic-Tac-Toe game\n",
        "\n",
        "class QLearningAgent:\n",
        "    def __init__(self, epsilon, alpha, gamma):\n",
        "        self.epsilon = epsilon  # Exploration rate\n",
        "        self.alpha = alpha  # Learning rate\n",
        "        self.gamma = gamma  # Discount factor\n",
        "        self.q_table = {}  # Q-value table\n",
        "\n",
        "    def get_action(self, state):\n",
        "        state_tuple = tuple(map(tuple, state))\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            valid_moves = [i for i in range(9) if state[i // 3][i % 3] == 0]\n",
        "            return random.choice(valid_moves)\n",
        "        else:\n",
        "            return max(\n",
        "                (i for i in range(9) if state[i // 3][i % 3] == 0),\n",
        "                key=lambda i: self.q_table.get(state_tuple, {}).get(i, 0),\n",
        "                default=random.choice([i for i in range(9) if state[i // 3][i % 3] == 0])\n",
        "            )\n",
        "\n",
        "    def learn(self, state, action, reward, next_state):\n",
        "        state_tuple = tuple(map(tuple, state))\n",
        "        next_state_tuple = tuple(map(tuple, next_state))\n",
        "\n",
        "        if state_tuple not in self.q_table:\n",
        "            self.q_table[state_tuple] = {}\n",
        "        if next_state_tuple not in self.q_table:\n",
        "            self.q_table[next_state_tuple] = {}\n",
        "\n",
        "        if action not in self.q_table[state_tuple]:\n",
        "            self.q_table[state_tuple][action] = 0\n",
        "\n",
        "        best_next_action = max(\n",
        "            (i for i in range(9) if next_state[i // 3][i % 3] == 0),\n",
        "            key=lambda i: self.q_table.get(next_state_tuple, {}).get(i, 0),\n",
        "            default=None\n",
        "        )\n",
        "\n",
        "        if best_next_action is not None:\n",
        "            self.q_table[state_tuple][action] += self.alpha * (\n",
        "                reward + self.gamma * self.q_table.get(next_state_tuple, {}).get(best_next_action, 0)\n",
        "                - self.q_table[state_tuple].get(action, 0)\n",
        "            )\n",
        "        else:\n",
        "            self.q_table[state_tuple][action] += self.alpha * (reward - self.q_table[state_tuple].get(action, 0))\n",
        "\n",
        "# 3. Train the reinforcement learning model\n",
        "def play_game(agent1, agent2, board):\n",
        "    state = board.copy()\n",
        "    while True:\n",
        "        action1 = agent1.get_action(state)\n",
        "        row, col = divmod(action1, 3)\n",
        "        state[row][col] = 1\n",
        "        if check_win(state, 1):\n",
        "            agent1.learn(state, action1, 1, state)\n",
        "            return 1  # Agent 1 wins\n",
        "        if check_draw(state):\n",
        "            return 0  # Draw\n",
        "        action2 = agent2.get_action(state)\n",
        "        row, col = divmod(action2, 3)\n",
        "        state[row][col] = -1\n",
        "        if check_win(state, -1):\n",
        "            agent1.learn(state, action1, -1, state)\n",
        "            return -1  # Agent 2 wins\n",
        "\n",
        "def train_q_learning_agents(agent1, agent2, num_episodes):\n",
        "    for episode in range(num_episodes):\n",
        "        board = np.zeros((3, 3), dtype=int)\n",
        "        if episode % 2 == 0:\n",
        "            result = play_game(agent1, agent2, board)\n",
        "            if result == 1:\n",
        "                agent1.learn(board, None, 1, board)\n",
        "                agent2.learn(board, None, -1, board)\n",
        "            elif result == -1:\n",
        "                agent1.learn(board, None, -1, board)\n",
        "                agent2.learn(board, None, 1, board)\n",
        "            else:\n",
        "                agent1.learn(board, None, 0, board)\n",
        "                agent2.learn(board, None, 0, board)\n",
        "        else:\n",
        "            result = play_game(agent2, agent1, board)\n",
        "            if result == 1:\n",
        "                agent1.learn(board, None, -1, board)\n",
        "                agent2.learn(board, None, 1, board)\n",
        "            elif result == -1:\n",
        "                agent1.learn(board, None, 1, board)\n",
        "                agent2.learn(board, None, -1, board)\n",
        "            else:\n",
        "                agent1.learn(board, None, 0, board)\n",
        "                agent2.learn(board, None, 0, board)\n",
        "\n",
        "# 4. Test the model\n",
        "if __name__ == \"__main__\":\n",
        "    agent1 = QLearningAgent(epsilon=0.2, alpha=0.1, gamma=0.9)\n",
        "    agent2 = QLearningAgent(epsilon=0.2, alpha=0.1, gamma=0.9)\n",
        "\n",
        "    # Train the agents\n",
        "    train_q_learning_agents(agent1, agent2, num_episodes=10000)\n",
        "\n",
        "    # Play a game between the trained agents\n",
        "    board = np.zeros((3, 3), dtype=int)\n",
        "    while True:\n",
        "        display_board(board)\n",
        "        action1 = agent1.get_action(board)\n",
        "        row, col = divmod(action1, 3)\n",
        "        board[row][col] = 1\n",
        "        if check_win(board, 1):\n",
        "            display_board(board)\n",
        "            print(\"Agent 1 wins!\")\n",
        "            break\n",
        "        if check_draw(board):\n",
        "            display_board(board)\n",
        "            print(\"It's a draw!\")\n",
        "            break\n",
        "        display_board(board)\n",
        "        action2 = agent2.get_action(board)\n",
        "        row, col = divmod(action2, 3)\n",
        "        board[row][col] = -1\n",
        "        if check_win(board, -1):\n",
        "            display_board(board)\n",
        "            print(\"Agent 2 wins!\")\n",
        "            break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICeQ4TYHUO8D",
        "outputId": "9ebd8f01-d13f-48b4-e364-732d832219ba"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------\n",
            "  |   |  \n",
            "---------\n",
            "  |   |  \n",
            "---------\n",
            "  |   |  \n",
            "---------\n",
            "---------\n",
            "X |   |  \n",
            "---------\n",
            "  |   |  \n",
            "---------\n",
            "  |   |  \n",
            "---------\n",
            "---------\n",
            "X | O |  \n",
            "---------\n",
            "  |   |  \n",
            "---------\n",
            "  |   |  \n",
            "---------\n",
            "---------\n",
            "X | O | X\n",
            "---------\n",
            "  |   |  \n",
            "---------\n",
            "  |   |  \n",
            "---------\n",
            "---------\n",
            "X | O | X\n",
            "---------\n",
            "O |   |  \n",
            "---------\n",
            "  |   |  \n",
            "---------\n",
            "---------\n",
            "X | O | X\n",
            "---------\n",
            "O | X |  \n",
            "---------\n",
            "  |   |  \n",
            "---------\n",
            "---------\n",
            "X | O | X\n",
            "---------\n",
            "O | X | O\n",
            "---------\n",
            "  |   |  \n",
            "---------\n",
            "---------\n",
            "X | O | X\n",
            "---------\n",
            "O | X | O\n",
            "---------\n",
            "X |   |  \n",
            "---------\n",
            "Agent 1 wins!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sure! Let me walk you through the code step by step.\n",
        "\n",
        "### Imports\n",
        "```python\n",
        "import numpy as np\n",
        "import random\n",
        "```\n",
        "- `numpy` is imported as `np` for handling arrays (especially for the board).\n",
        "- `random` is imported for making random decisions (used in the exploration step of Q-learning).\n",
        "\n",
        "### Board Display Function\n",
        "```python\n",
        "def display_board(board):\n",
        "    for row in board:\n",
        "        print(\"-\" * 9)\n",
        "        print(\" | \".join([\"X\" if cell == 1 else \"O\" if cell == -1 else \" \" for cell in row]))\n",
        "    print(\"-\" * 9)\n",
        "```\n",
        "- **`display_board(board)`** prints the Tic-Tac-Toe board in a human-readable format.\n",
        "  - It loops through each row of the board (a 2D array).\n",
        "  - It prints a separator line `\"-\" * 9` for visual clarity.\n",
        "  - The row is printed with `\"X\"` for 1, `\"O\"` for -1, and `\" \"` for empty cells (`0`).\n",
        "  - Finally, another separator line is printed after the last row.\n",
        "\n",
        "### Valid Move Checker\n",
        "```python\n",
        "def is_valid_move(board, row, col):\n",
        "    return board[row][col] == 0\n",
        "```\n",
        "- **`is_valid_move(board, row, col)`** checks whether a move is valid by ensuring the selected cell is empty (`0`).\n",
        "\n",
        "### Win Checker\n",
        "```python\n",
        "def check_win(board, player):\n",
        "    # Check rows, columns, and diagonals for a win\n",
        "    for row in board:\n",
        "        if all(cell == player for cell in row):\n",
        "            return True\n",
        "    for col in board.T:\n",
        "        if all(cell == player for cell in col):\n",
        "            return True\n",
        "    if all(board[i][i] == player for i in range(3)) or all(board[i][2 - i] == player for i in range(3)):\n",
        "        return True\n",
        "    return False\n",
        "```\n",
        "- **`check_win(board, player)`** checks if the given player (1 for \"X\" and -1 for \"O\") has won.\n",
        "  - It checks each row, each column (using the transpose of the board), and both diagonals.\n",
        "  - If any row, column, or diagonal is fully occupied by the player’s symbol (`1` or `-1`), it returns `True`, meaning the player has won.\n",
        "\n",
        "### Draw Checker\n",
        "```python\n",
        "def check_draw(board):\n",
        "    return np.all(board != 0)\n",
        "```\n",
        "- **`check_draw(board)`** returns `True` if the board is full (i.e., no cells contain `0`), meaning the game has ended in a draw.\n",
        "\n",
        "### Q-Learning Agent Class\n",
        "```python\n",
        "class QLearningAgent:\n",
        "    def __init__(self, epsilon, alpha, gamma):\n",
        "        self.epsilon = epsilon  # Exploration rate\n",
        "        self.alpha = alpha  # Learning rate\n",
        "        self.gamma = gamma  # Discount factor\n",
        "        self.q_table = {}  # Q-value table\n",
        "```\n",
        "- **`QLearningAgent`** is a class that represents a Q-learning agent.\n",
        "  - `epsilon`: The exploration rate, determining the likelihood of choosing a random move over a learned move.\n",
        "  - `alpha`: The learning rate, which determines how much new experiences affect the agent's learning.\n",
        "  - `gamma`: The discount factor, which determines the importance of future rewards in the agent’s decision-making.\n",
        "  - `q_table`: A dictionary to store the Q-values (the quality of each action in a given state).\n",
        "\n",
        "### Action Selection (Exploration vs Exploitation)\n",
        "```python\n",
        "def get_action(self, state):\n",
        "    state_tuple = tuple(map(tuple, state))\n",
        "    if np.random.rand() < self.epsilon:\n",
        "        valid_moves = [i for i in range(9) if state[i // 3][i % 3] == 0]\n",
        "        return random.choice(valid_moves)\n",
        "    else:\n",
        "        return max(\n",
        "            (i for i in range(9) if state[i // 3][i % 3] == 0),\n",
        "            key=lambda i: self.q_table.get(state_tuple, {}).get(i, 0),\n",
        "            default=random.choice([i for i in range(9) if state[i // 3][i % 3] == 0])\n",
        "        )\n",
        "```\n",
        "- **`get_action(self, state)`** determines which action (move) the agent should take.\n",
        "  - It first converts the board (state) to a tuple to be used as a key in the `q_table`.\n",
        "  - If a random number is less than `epsilon`, it explores by choosing a random valid move.\n",
        "  - Otherwise, it exploits its learned knowledge and selects the action with the highest Q-value from the `q_table`. If there are no Q-values, it defaults to choosing a random move.\n",
        "\n",
        "### Learning from Experience\n",
        "```python\n",
        "def learn(self, state, action, reward, next_state):\n",
        "    state_tuple = tuple(map(tuple, state))\n",
        "    next_state_tuple = tuple(map(tuple, next_state))\n",
        "\n",
        "    if state_tuple not in self.q_table:\n",
        "        self.q_table[state_tuple] = {}\n",
        "    if next_state_tuple not in self.q_table:\n",
        "        self.q_table[next_state_tuple] = {}\n",
        "\n",
        "    if action not in self.q_table[state_tuple]:\n",
        "        self.q_table[state_tuple][action] = 0\n",
        "\n",
        "    best_next_action = max(\n",
        "        (i for i in range(9) if next_state[i // 3][i % 3] == 0),\n",
        "        key=lambda i: self.q_table.get(next_state_tuple, {}).get(i, 0),\n",
        "        default=None\n",
        "    )\n",
        "\n",
        "    if best_next_action is not None:\n",
        "        self.q_table[state_tuple][action] += self.alpha * (\n",
        "            reward + self.gamma * self.q_table.get(next_state_tuple, {}).get(best_next_action, 0)\n",
        "            - self.q_table[state_tuple].get(action, 0)\n",
        "        )\n",
        "    else:\n",
        "        self.q_table[state_tuple][action] += self.alpha * (reward - self.q_table[state_tuple].get(action, 0))\n",
        "```\n",
        "- **`learn(self, state, action, reward, next_state)`** updates the Q-values based on the agent’s experience.\n",
        "  - It ensures both `state_tuple` and `next_state_tuple` exist in the `q_table`.\n",
        "  - If the `action` has no Q-value, it initializes it to `0`.\n",
        "  - The agent calculates the best next action's Q-value and updates the Q-value for the current action using the Q-learning formula:\n",
        "    \\[\n",
        "    Q(s, a) \\leftarrow Q(s, a) + \\alpha \\times \\left[ R + \\gamma \\times \\max_{a'}Q(s', a') - Q(s, a) \\right]\n",
        "    \\]\n",
        "  - If no valid next action exists, it simply updates the Q-value based on the reward.\n",
        "\n",
        "### Game Play Function\n",
        "```python\n",
        "def play_game(agent1, agent2, board):\n",
        "    state = board.copy()\n",
        "    while True:\n",
        "        action1 = agent1.get_action(state)\n",
        "        row, col = divmod(action1, 3)\n",
        "        state[row][col] = 1\n",
        "        if check_win(state, 1):\n",
        "            agent1.learn(state, action1, 1, state)\n",
        "            return 1  # Agent 1 wins\n",
        "        if check_draw(state):\n",
        "            return 0  # Draw\n",
        "        action2 = agent2.get_action(state)\n",
        "        row, col = divmod(action2, 3)\n",
        "        state[row][col] = -1\n",
        "        if check_win(state, -1):\n",
        "            agent1.learn(state, action1, -1, state)\n",
        "            return -1  # Agent 2 wins\n",
        "```\n",
        "- **`play_game(agent1, agent2, board)`** simulates a game between two Q-learning agents (`agent1` and `agent2`).\n",
        "  - Each agent takes turns selecting a move and updating the board.\n",
        "  - If any agent wins, the game ends and the winning agent is rewarded.\n",
        "  - If the game ends in a draw, a reward of 0 is given.\n",
        "  - After each move, the agent learns from the current state, action, and reward.\n",
        "\n",
        "### Training the Agents\n",
        "```python\n",
        "def train_q_learning_agents(agent1, agent2, num_episodes):\n",
        "    for episode in range(num_episodes):\n",
        "        board = np.zeros((3, 3), dtype=int)\n",
        "        if episode % 2 == 0:\n",
        "            result = play_game(agent1, agent2, board)\n",
        "            if result == 1:\n",
        "                agent1.learn(board, None, 1, board)\n",
        "                agent2.learn(board, None, -1, board)\n",
        "            elif result == -1:\n",
        "                agent1.learn(board, None, -1, board)\n",
        "                agent2.learn(board, None, 1, board)\n",
        "            else:\n",
        "                agent1.learn(board, None, 0, board)\n",
        "                agent2.learn(board, None, 0, board)\n",
        "        else:\n",
        "            result = play_game(agent2, agent1, board)\n",
        "            if result == 1:\n",
        "                agent1.learn(board, None, -1, board)\n",
        "                agent2.learn(board, None, 1, board)\n",
        "            elif result == -1:\n",
        "                agent1.learn(board, None, 1, board)\n",
        "                agent2.learn(board, None, -1, board)\n",
        "            else:\n",
        "                agent1.learn(board, None, 0, board)\n",
        "                agent2.learn(board, None, 0, board)\n",
        "```\n",
        "- **`train_q_learning_agents(agent1, agent2, num_episodes)`** trains the agents over multiple episodes.\n",
        "  - In each episode, the agents play a game and learn from the outcome.\n",
        "  -\n",
        "\n",
        " Depending on whether `episode % 2 == 0`, the order of agents is swapped.\n",
        "  - After each game, the agents learn from the result (win, loss, or draw).\n",
        "\n",
        "### Main Loop\n",
        "```python\n",
        "if __name__ == \"__main__\":\n",
        "    epsilon = 0.1\n",
        "    alpha = 0.5\n",
        "    gamma = 0.9\n",
        "    agent1 = QLearningAgent(epsilon, alpha, gamma)\n",
        "    agent2 = QLearningAgent(epsilon, alpha, gamma)\n",
        "    num_episodes = 10000\n",
        "    train_q_learning_agents(agent1, agent2, num_episodes)\n",
        "```\n",
        "- **`__main__`** initializes two agents (`agent1` and `agent2`).\n",
        "  - It sets the Q-learning parameters: `epsilon`, `alpha`, and `gamma`.\n",
        "  - It trains the agents for 10,000 episodes using `train_q_learning_agents`.\n",
        "\n",
        "This implementation sets up and trains two Q-learning agents to play Tic-Tac-Toe. The agents learn optimal strategies through repeated games and experience. The learning process updates the Q-values and adjusts the agents' decision-making based on the outcome of each game."
      ],
      "metadata": {
        "id": "T8MkRwWBVklZ"
      }
    }
  ]
}